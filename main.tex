\documentclass{acaces}

\begin{document}


\title{Data Aware\\Embedded Machine Learning
}

\author{
    Emil~Njor\addressnum{1}\extranum{1}
}

\address{1}{
    Danmarks Tekniske Universitet,
    Richard Petersens Plads,
    Bygning 322,
    Rum 111,
    2800 Kongens Lyngby,
    Denmark
}

\extra{1}{E-mail: emjn@dtu.dk}

\pagestyle{empty}


\begin{abstract}
    Abstract to be filled out
    
\end{abstract}

\keywords{Embedded Machine Learning, Data Aware Methods, Data Aware NAS, Data Aware Datasets, Data Aware Neural Network}

\section{Introduction}
Embedded Machine Learning, also known as tinyML, is a research field aiming to bring complex machine learning models to embedded devices. 
The field is still in its infancy, but has already shown promising results in a wide range of applications, such as keyword spotting, image classification, and predictive maintenance.

The main challenge of embedded machine learning is to make machine learning work in the constrained resources, such as memory, compute, and energy, of embedded devices.
Most research in the field has focused on reducing the resource consumption of the machine learning models themselves - especially resource hungry models such as neural networks.

This reduction of resource consumption has been achieved using methods such as quantization, pruning and neural architecture search\cite{njor2022primer}.

However, the resource consumption of the machine learning model is only a part of the resource consumption of the entire machine learning system.
A typical embedded machine learning tasks consists of collecting data from a sensor, pre-processing this data, running the machine learning model, and taking some action based on the output of the machine learning model.

In my research I work with the data granularity of the input data captured, pre-processed and given to the machine learning model.
At the core of this research is the idea that input data can be configured to be more or less resource intensive.

E.g., sound data can be captured at different sample rates.
Likewise, image data can be captured at different resolutions.
Higher sample rates and image resolutions will be more resource intensive, but will also contain more information that may be valuable to the machine learning model.
On the other hand, lower sample rates and image resolutions will be less resource intensive, but will also contain less information.

This idea can be used to reduce the resource consumption of an embedded machine learning system, but also to improve the performance of a machine learning system under some given resource constraints.
We have thought the name "Data Aware Embedded Machine Learning" suitable to describe this research direction.

In the following sections of this poster abstract I will describe my work and ideas for work in this research direction in more detail.
I will introduce the ideas of:
\begin{description}
    \item[Data Aware Neural Architecture Search] An extension of Neural Architecture Search that simultaneously searches for an optimal combination of a Neural Network Architecture and a Data Granularity.
    \item[Data Aware Datasets] Datasets that support easy loading of multiple data granularities.
    \item[Data Aware Neural Networks] Specialized Neural Networks that can run inference of multiple data granularities.
\end{description}

\section{Data Aware Neural Architecture Search}
The idea of a Data Aware Neural Architecture Search builds on top of a Neural Architecture Search.
Simply put, a Neural Architecture Search is a computer program that searches for an optimal Neural Network Architecture for a given task.
A Neural Architecture Search is defined by a search space of possible Neural Network Architectures, a search strategy that defines how to search the search space, and a performance metric that defines how to evaluate the performance of a given Neural Network Architecture.

A Data Aware Neural Architecture Search extends this by also searching for an optimal Data Granularity for a given task.

A Data Aware Neural Architecture Search 

% Advantages

% Disadvantages
% Larger Search Space

\section{Data Aware Datasets}
% Many datasets can relatively simply be loaded at different data granularities
% Ease of use is important for adoption

\section{Data Aware Neural Networks}
% Per sample data granularity

\bibliography{bibliography}

\end{document}

